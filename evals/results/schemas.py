"""Result schemas for evaluation runs.

This module defines schemas for storing and analyzing evaluation results:
- Individual example results (EvalResult)
- Aggregate statistics across runs (AggregateMetrics)
- Full run results with breakdowns (EvalRunResults)

All schemas use Pydantic BaseModel for JSON serialization and validation.
"""

from datetime import datetime
from statistics import mean, median, stdev
from typing import Any

from pydantic import BaseModel, Field, computed_field

from evals.metrics.retrieval import RetrievalMetrics
from evals.schemas.task import DifficultyLevel, QuestionType


class AggregateStats(BaseModel):
    """Aggregate statistics for a metric across multiple examples.

    Provides comprehensive statistical summary including mean, standard
    deviation, min, max, and median.
    """

    mean: float = Field(..., description="Arithmetic mean of the metric")
    std: float = Field(..., ge=0.0, description="Standard deviation of the metric")
    min: float = Field(..., description="Minimum value observed")
    max: float = Field(..., description="Maximum value observed")
    median: float = Field(..., description="Median (50th percentile) value")
    count: int = Field(..., ge=0, description="Number of examples")

    @classmethod
    def from_values(cls, values: list[float]) -> "AggregateStats":
        """Create aggregate stats from a list of values.

        Args:
            values: List of metric values

        Returns:
            AggregateStats with computed statistics

        Raises:
            ValueError: If values is empty
        """
        if not values:
            raise ValueError("Cannot compute aggregate stats from empty list")

        return cls(
            mean=mean(values),
            std=stdev(values) if len(values) > 1 else 0.0,
            min=min(values),
            max=max(values),
            median=median(values),
            count=len(values),
        )


class EvalResult(BaseModel):
    """Result for a single evaluation example.

    Contains the question, generated answer, retrieved chunks, metrics,
    and full execution trace for debugging and analysis.
    """

    # Task identification
    eval_id: str = Field(..., description="Unique ID from EvalTask (e.g., 'eval_001')")
    question: str = Field(..., description="The question that was asked")
    question_type: QuestionType = Field(
        ..., description="Type of question (factual, analytical, opinion)"
    )
    difficulty_level: DifficultyLevel = Field(
        ..., description="Difficulty level (easy, medium, hard)"
    )

    # Ground truth
    reference_answer: str = Field(
        ..., description="Ground truth reference answer from EvalTask"
    )
    expected_chunk_ids: list[int] = Field(
        ..., description="Expected/relevant chunk IDs from EvalTask.source_chunk_ids"
    )

    # Generated output
    generated_answer: str = Field(
        ..., description="Answer generated by the RAG agent"
    )
    retrieved_chunk_ids: list[int] = Field(
        ..., description="List of chunk IDs retrieved for this question (in rank order)"
    )

    # Metrics per k value
    metrics_by_k: dict[int, RetrievalMetrics] = Field(
        ..., description="Retrieval metrics computed for each k value"
    )

    # Execution metadata
    latency_ms: float = Field(
        ..., ge=0, description="Total agent execution time in milliseconds"
    )
    model_used: str = Field(..., description="LLM model used for answer generation")
    tokens_used: dict[str, int] = Field(
        ...,
        description="Token usage breakdown (prompt_tokens, completion_tokens, total_tokens)",
    )
    trace_id: str | None = Field(
        None, description="LangSmith trace ID for debugging (if available)"
    )

    # Agent-specific metadata (optional)
    sub_queries: list[str] | None = Field(
        None, description="Sub-queries used (multi-query agent only)"
    )
    deduplication_stats: dict[str, Any] | None = Field(
        None, description="Deduplication stats (multi-query agent only)"
    )

    # Success/error tracking
    success: bool = Field(True, description="Whether evaluation succeeded")
    error: str | None = Field(None, description="Error message if evaluation failed")

    # Timestamp
    executed_at: datetime = Field(
        default_factory=datetime.now, description="When this example was evaluated"
    )


class MetricsBreakdown(BaseModel):
    """Aggregate metrics for a subset of examples."""

    recall: AggregateStats = Field(..., description="Recall@k aggregate statistics")
    precision: AggregateStats = Field(
        ..., description="Precision@k aggregate statistics"
    )
    hit_rate: AggregateStats = Field(..., description="Hit rate aggregate statistics")
    mrr: AggregateStats = Field(..., description="MRR aggregate statistics")
    ndcg: AggregateStats = Field(..., description="NDCG@k aggregate statistics")
    latency_ms: AggregateStats = Field(..., description="Latency aggregate statistics")
    count: int = Field(..., description="Number of examples in this breakdown")


class EvalRunResults(BaseModel):
    """Complete evaluation run results with aggregates and breakdowns.

    Contains all individual results plus aggregate statistics overall
    and broken down by difficulty level and question type.
    """

    # Run metadata
    run_id: str = Field(
        ..., description="Unique identifier for this evaluation run"
    )
    agent_type: str = Field(
        ..., description="Agent type used (e.g., 'vanilla', 'multi-query')"
    )
    dataset_path: str = Field(..., description="Path to the eval dataset used")
    dataset_version: str = Field(..., description="Version of the eval dataset")

    # Retrieval configuration
    retrieval_mode: str = Field(
        ..., description="Retrieval mode used (e.g., 'fts', 'vector', 'hybrid')"
    )
    fts_candidates: int = Field(
        ..., description="Number of FTS candidates (for hybrid mode)"
    )
    max_returned: int = Field(
        ..., description="Number of chunks returned after reranking"
    )
    k_values: list[int] = Field(
        ..., description="K values used for computing @k metrics"
    )

    # Timestamps
    started_at: datetime = Field(..., description="When the evaluation run started")
    completed_at: datetime = Field(..., description="When the evaluation run completed")

    # Individual results
    results: list[EvalResult] = Field(
        ..., description="List of individual evaluation results"
    )

    # Overall aggregates per k value
    overall_by_k: dict[int, MetricsBreakdown] = Field(
        ..., description="Overall aggregate metrics for each k value"
    )

    # Breakdowns by difficulty (per k value)
    by_difficulty: dict[str, dict[int, MetricsBreakdown]] = Field(
        default_factory=dict,
        description="Metrics broken down by difficulty level and k",
    )

    # Breakdowns by question type (per k value)
    by_question_type: dict[str, dict[int, MetricsBreakdown]] = Field(
        default_factory=dict,
        description="Metrics broken down by question type and k",
    )

    # Error tracking
    num_successful: int = Field(..., description="Number of successful evaluations")
    num_failed: int = Field(..., description="Number of failed evaluations")
    errors: list[dict[str, str]] = Field(
        default_factory=list, description="List of errors with eval_id and message"
    )

    # Computed properties
    @computed_field
    @property
    def total_examples(self) -> int:
        """Total number of examples evaluated."""
        return len(self.results)

    @computed_field
    @property
    def success_rate(self) -> float:
        """Fraction of successful evaluations."""
        if self.total_examples == 0:
            return 0.0
        return self.num_successful / self.total_examples

    @computed_field
    @property
    def total_duration_seconds(self) -> float:
        """Total evaluation run duration in seconds."""
        return (self.completed_at - self.started_at).total_seconds()


def build_metrics_breakdown(
    results: list[EvalResult],
    k: int,
) -> MetricsBreakdown:
    """Build aggregate metrics breakdown from a list of results.

    Args:
        results: List of evaluation results
        k: The k value to use for metrics

    Returns:
        MetricsBreakdown with aggregate statistics
    """
    successful = [r for r in results if r.success and k in r.metrics_by_k]

    if not successful:
        # Return zeros if no successful results
        zero_stats = AggregateStats(
            mean=0.0, std=0.0, min=0.0, max=0.0, median=0.0, count=0
        )
        return MetricsBreakdown(
            recall=zero_stats,
            precision=zero_stats,
            hit_rate=zero_stats,
            mrr=zero_stats,
            ndcg=zero_stats,
            latency_ms=zero_stats,
            count=0,
        )

    recalls = [r.metrics_by_k[k].recall_at_k for r in successful]
    precisions = [r.metrics_by_k[k].precision_at_k for r in successful]
    hit_rates = [r.metrics_by_k[k].hit_rate for r in successful]
    # For MRR, filter out None values (no match) - they shouldn't contribute to mean
    mrrs: list[float] = []
    for r in successful:
        mrr_val = r.metrics_by_k[k].mrr
        if mrr_val is not None:
            mrrs.append(mrr_val)
    # If all MRR values are None, use a single 0.0 to avoid empty list
    if not mrrs:
        mrrs = [0.0]
    ndcgs = [r.metrics_by_k[k].ndcg_at_k for r in successful]
    latencies = [r.latency_ms for r in successful]

    return MetricsBreakdown(
        recall=AggregateStats.from_values(recalls),
        precision=AggregateStats.from_values(precisions),
        hit_rate=AggregateStats.from_values(hit_rates),
        mrr=AggregateStats.from_values(mrrs),
        ndcg=AggregateStats.from_values(ndcgs),
        latency_ms=AggregateStats.from_values(latencies),
        count=len(successful),
    )
